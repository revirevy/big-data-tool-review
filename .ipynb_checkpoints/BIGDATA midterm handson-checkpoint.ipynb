{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## LINUX (Hands on) (See Linux.pdf instead)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "* file listing\n",
    "![](img_midterm/ls.png)\n",
    "* change directories\n",
    "![](img_midterm/cd.png)\n",
    "\n",
    "* copy \n",
    "![](img_midterm/cp.png)\n",
    "* move \n",
    "![](img_midterm/mv.png)\n",
    "* delete\n",
    "![](img_midterm/rm.png)\n",
    "* find \n",
    "![](img_midterm/find.png)\n",
    "* create/remove directories, \n",
    "![](img_midterm/mkdir.png)\n",
    "* sample/view large text files, \n",
    "![](img_midterm/view_larger.png)\n",
    "\n",
    "\n",
    "* pipe \n",
    "* redirection, \n",
    "* head, tail, \n",
    "![](img_midterm/head.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HDFS(hands on) (from hw1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## MapReduce (hands on)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "* create pseudo codes for map and reduce functions for relative simple MapReduceproblems (that can be done with one MapReduce)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "* Must use hadoop specific utilities or custom code to access HDFS\n",
    "```\n",
    "hadoop fs\n",
    "```\n",
    "\n",
    "* file listing \n",
    "\n",
    "```\n",
    "# list file at hdfs root directory\n",
    "hadoop fs -ls /\n",
    "\n",
    "#list file at hdfs home directory\n",
    "hadoop fs -ls\n",
    "```\n",
    "\n",
    "* uploading / downloading from local filesystem\n",
    "![](img_midterm/cpfromtolocal.png)\n",
    "\n",
    "* deletion\n",
    "> Hadoop won't overwrite files and directories. This feature helps protect users from accidentally replacing data that may have taken hours to produce. If you need to replace a file or directory in HDFS, you must first remove the existing one. \n",
    "\n",
    "```\n",
    "#remove a file\n",
    "$ hadoop fs -rm /dualcore/example.txt\n",
    "```\n",
    "* create/remove directory\n",
    "\n",
    "```\n",
    "#this will create a dir testlog at the home directory\n",
    "hadoop fs -mkdir testlog\n",
    "\n",
    "#remove a directory (recursively):\n",
    "hadoop fs -rm -r /dualcore/example/\n",
    "```\n",
    "* sample data (e.g., head/tail)(hw1 15)\n",
    "\n",
    "```\n",
    "hadoop fs -cat latlon/latlon.tsv | head -n 10\n",
    "```\n",
    "\n",
    "* viewing data (cat) (hw1 15)\n",
    "\n",
    "```\n",
    "hadoop fs -cat latlon/latlon.tsv\n",
    "```\n",
    "* extract data (get and getmerge)(hw1 18)\n",
    "\n",
    "```\n",
    "#HW1 18: Download latlon.tsv from HDFS as latlon_hdfs.tsv in the local folder ADIR/data\n",
    "\n",
    "hadoop fs -get latlon/latlon_hdfs.tsv ADIR/data/latlon_hdfs.tsv\n",
    "\n",
    "#HW1 19: Download the entire content of the latlon folder in HDFS as a single local file latlon_all_hdfs.tsv in the folder ADIR/data\n",
    "\n",
    "hadoop fs -getmerge latlon latlon_all_hdfs.tsv\n",
    "\n",
    "```\n",
    "* count # of rows, **columns**\n",
    "\n",
    "```\n",
    "\n",
    "hadoop fs -cat /path/to/hdfs/filename | wc -l\n",
    "\n",
    "```\n",
    "unzip and upload to hdfs. (hw1)\n",
    "\n",
    "```\n",
    "#gunzip is used for uncompress file compressed by gzip command, the file name ends with .gz, \n",
    "#The -c option to gunzip uncompresses to standard output, and the dash (-) in the hadoop fs -put command takes whatever is being sent to its standard input and places that data in HDFS.\n",
    "\n",
    "gunzip -c ~/training_materials/developer/data/access_log.gz \\\n",
    "| hadoop fs -put - weblog/access_log\n",
    "\n",
    "#unzip is used for uncompress file compressed by zip command, the file name ends with .zip, \n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sqoop: (Hands on) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Log in to MYSQL\n",
    "\n",
    "```\n",
    "$ mysql -uroot -pcloudera dualcore \n",
    "#or \n",
    "$ mysql --user=root --password=cloudera dualcore\n",
    "```\n",
    "\n",
    "* Import MySQL tables into Hadoop (Lab 3 no.5,Topic3-sqoop pg5 )\n",
    "    *  including using options (e.g., select table, rows, and columns; specify destination location, delimiters,\n",
    "split by field; \n",
    "```\n",
    "$ sqoop import \\\n",
    " --connect jdbc:mysql://localhost/dualcore \\\n",
    " --username root --password cloudera \\\n",
    " --fields-terminated-by '\\t' \\\n",
    " --warehouse-dir /dualcore \\ # sepcific destination (by default your home directory)\n",
    " --table employees\n",
    " --columns \"prod_id,name,price\" # imported multiple column separated by comma\n",
    " --where \"price >= 1000\" #imported specific rows\n",
    " --split by 'id '-m 10 (split data to 10-fold by id) (doesn't work well with non-numeric data)\n",
    "```\n",
    "* Import MySQL tables into Hive\n",
    "    * you can control the output table name with the --hive-table option\n",
    "\n",
    "![](img_midterm/sqooptohive.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hive (Hands on) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Write standard Hive queries (Hive2-DataAnalysis.pdf pg10-pg12).\n",
    "    * Subqueries in the from and where clause.(Hive2-DataAnalysis.pdf pg10-pg12).\n",
    "    * Joins IN Hive (Hive2-DataAnalysis.pdf pg20-pg21).\n",
    "\n",
    "* Be able to retrieve meta data – describe [formatted]\n",
    "table, show databases, show tables; use database. (Hive2-DataAnalysis.pdf pg6-pg7)\n",
    "```\n",
    "SHOW DATABASES;\n",
    "USE dualcores;\n",
    "SHOW TABLES;\n",
    "DESCRIBE orders;\n",
    "DESCRIBE FORMATTED orders (provides more detailed information)\n",
    "```\n",
    "\n",
    "* Be able to use common text functions (e.g., SUBSTR, LENGTH, TRIM, CONCAT, CONCAT_WS,\n",
    "CAST, UPPER/LOWER, EXPLODE, SPLIT) (Hive5.pdf pg5)\n",
    "\n",
    "\n",
    "* create managed tables / external tables(Hive3.pdf pg11), specifying field data types, table\n",
    "delimiters, location, storage formats. (Hive3.pdf pg6-pg10)\n",
    "\n",
    "\n",
    "* Use of OpenCSVSerDe (Hive5.pdf pg22)\n",
    "\n",
    "* different ways of loading data into Hive tables. LOAD DATA [LOCAL]\n",
    "INPATH(pg15-pg17), Sqoop hive import(pg18), CREATE TABLE … AS SELECT(pg24); INSERT INTO\n",
    "TABLE … SELECT (pg23).(Hive3.pdf)\n",
    "\n",
    "* Be able to convert table from one storage format into another. (Hive4.pdf pg15)\n",
    "\n",
    "* Be able to locate directory that stores a table's data. (Describe formatted Tablename)\n",
    "\n",
    "* Create and load partitioned tables (using static or dynamic partitions). (Hive4.pdf pg18)\n",
    "\n",
    "* Optimize hive table with binary storage formats.\n",
    "\n",
    "* Be able to query complex fields (maps, structs, and arrays). (Lab 7x)(Lab 8)\n",
    "\n",
    "* Use of Hive text functions (SPLIT, EXPLODE, SENTENCES, TRIM,\n",
    "NGRAMS,historgram_numeric), REGEXP,REGEXP_EXTRACT (regular expression is limited to patterns introduced at the level of our in-class examples) (Hive5.pdf,pg24-pg28)\n",
    "\n",
    "* Extract information from JSON document using GET_JSON_OBJECT()\n",
    "(limited to extracting a single value). (Hive6.pdf)\n",
    "\n",
    "* Create a table with complex fields, including using JsonSerDe. (Lab 7x)(Lab 12 for JsonSerde)(Hive6.pdf for json)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
