{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Big data: Conceptual: \n",
    "* Concept of Big Data:\n",
    "\n",
    "\n",
    "* drivers of big data\n",
    "\n",
    "* five Vs\n",
    "    * Volume, \n",
    "        * dataset bewteen Terabyte and Petabyte is \"big\"\n",
    "        * opportunity: large volume raise the chance of getting better data\n",
    "        * challenge: reading data takes a lot of time (parallel computing using Hadoop)\n",
    "    * Velocity (Data generated/processed/analyzed per second/hour/day)\n",
    "        * Speed at which data is created, processed and analyzed continues to accelerate\n",
    "        * challenge: processing log data (streaming analytics using Spark)\n",
    "    * Variety: complexity of data types,and sources\n",
    "        * different structure of data\n",
    "        * Internal source: transactional systems,server logs, emails\n",
    "        * External: social media, sensor networks, weather data\n",
    "        * Opportunity: Get high quality data, Big data provides a way to capture novel data sources like social media,click stream\n",
    "        * Challenge: traditional technologies are not flexible to deal with large variety of semi- or un-structured data (80% are unstructured, while 20% are structured.)\n",
    "    * Verasity: data uncertainty. Large volumes means higher risks of getting incorrect information. Incorrectly indexed data or spelling mistake could make whole dataset useless.\n",
    "    * Value: valuable application.\n",
    "        * As volumn increases, value of each extra data point decrease.\n",
    "        * As variety of data available increases, not all data may have value.\n",
    "\n",
    "![img_5v](img_midterm\\5v.png)\n",
    "\n",
    "\n",
    "* different data structure\n",
    "    * structured: transactional\n",
    "    * Semi-structured: sensor data,logs,RFID\n",
    "    * Unstructured: reviews, texts, audio,video\n",
    "    \n",
    "    \n",
    "* data size units\n",
    "![datasize](img_midterm\\datasize.png)\n",
    "\n",
    "\n",
    "* the data transfer problem, and how/why big data can help.\n",
    "![tranfer1](img_midterm\\datatransfer1.png)\n",
    "transfer\n",
    "![tranfer2](img_midterm\\datatransfer2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Linux OS: \n",
    "* linux command structure:\n",
    "![lcs](img_midterm/lcs.png)\n",
    "\n",
    "linux file structure: Tree structure,\n",
    "* root: /\n",
    "* home: ~ (default directory)\n",
    "* parent dir: ../\n",
    "\n",
    "\n",
    "![](img_midterm/linux_structure.png)\n",
    "\n",
    "\n",
    "\n",
    "* relative and absolute path:\n",
    "\n",
    "    * abs path: start with root(/) \n",
    "```\n",
    "cd /xx\n",
    "```\n",
    "\n",
    "    * relative path: no /\n",
    "```\n",
    "cd xx\n",
    "```\n",
    "\n",
    "* pipe and redirection\n",
    "\n",
    "![](img_midterm/pipe.png)\n",
    "\n",
    " - Shell: interactive command interpreter environment (command line interface) for access to an operating system\n",
    " - Terminal: a terminal window allow users to access to shell. multiple terminal window can access to the local machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LINUX (Hands on) (See Linux.pdf instead)\n",
    "\n",
    "* file listing\n",
    "![](img_midterm/ls.png)\n",
    "* change directories\n",
    "![](img_midterm/cd.png)\n",
    "\n",
    "* copy \n",
    "![](img_midterm/cp.png)\n",
    "* move \n",
    "![](img_midterm/mv.png)\n",
    "* delete\n",
    "![](img_midterm/rm.png)\n",
    "* find \n",
    "![](img_midterm/find.png)\n",
    "* create/remove directories, \n",
    "![](img_midterm/mkdir.png)\n",
    "* sample/view large text files, \n",
    "![](img_midterm/view_larger.png)\n",
    "\n",
    "\n",
    "* pipe \n",
    "* redirection, \n",
    "* head, tail, \n",
    "![](img_midterm/head.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro to Hadoop: Conceptual: \n",
    "\n",
    "* motivation for Hadoop \n",
    "    * Storing large scale of data is cheaper than before, but processing data is hard for traditional tools like SQL.\n",
    "        * Hardware failures (how can we reliably store big data at a reasonable cost )\n",
    "        * time needed to read the data into memory for analysis.(how to analyze the big data)\n",
    "    * Hadoop deal with these problem:\n",
    "        * it provides reliable distributed storage, and a general framework for parallel processing at low cost\n",
    "        \n",
    "* core hadoop components (both are scalable, reliable, available, fast)\n",
    "    * Storage: Hadoop Distributed File System (HDFS)\n",
    "        *A framework for distributing data across a cluster\n",
    "    * Processing: MapReduce (A framework for processing data)\n",
    "    * other infrastrucutres to make it work\n",
    "    \n",
    "* Hadoop cluster:\n",
    "    * cluster: a collection of servers running Hadoop software\n",
    "    * nodes: Individual servers within a cluster\n",
    "        * data locality: Each node both stores and processes data\n",
    "* how hadoop achieves scalability, reliability/availability\n",
    "    * scalability: add more nodes to increase scalability (linearly)\n",
    "        * horizontal scaling results in lower cost (run on common hardware)\n",
    "        * horizontal scaling: scale by adding more machine\n",
    "        * Vertical scaling: scale by increasing the capacity of an existing machine\n",
    "    * reliability\n",
    "        * Fault Tolerance below\n",
    "    * **availability**\n",
    "* fault tolerance: \n",
    "![](img_midterm/fault_tolerance.png)\n",
    "    \n",
    "\n",
    "* **differences between Hadoop and RDBMS;**\n",
    "* **advantages & disadvantages of Hadoop**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HDFS (Conceptual): \n",
    "\n",
    "* how HDFS organizes data\n",
    "    * provides inexpensive and reliable storage for massive amounts of data\n",
    "    * Like UNIX, tree structure file system\n",
    "        * But no \"current directory\" need to use absolute path\n",
    "    * Optimized for a relatively small number of large files (> 100mb, maybe multiple gbs)\n",
    "![](img_midterm/optimal_for_small_number.png) \n",
    "    \n",
    "* HDFS Architecture:\n",
    "    * Blocks: Smallest unit for read and write. HDFS files are broken into blocks. By default, HDFS uses 128MB blocks.\n",
    "![](img_midterm/blocks.png)\n",
    "    * Master/Slave architecture:\n",
    "![](img_midterm/master_slave.png)\n",
    "![](img_midterm/master_slave_ex.png)\n",
    "    \n",
    "* how data is replicated and distributed within HDFS \n",
    "    * Each blocks is replicated multiple times and stored in different nodes of the cluster.\n",
    "\n",
    "\n",
    "* immutability (cannot change after writing).\n",
    "    * cannot modify files once written, but you can delete and recreate them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HDFS(hands on) \n",
    "* Must use hadoop specific utilities or custom code to access HDFS\n",
    "```\n",
    "hadoop fs\n",
    "```\n",
    "\n",
    "* file listing \n",
    "\n",
    "```\n",
    "# list file at hdfs root directory\n",
    "hadoop fs -ls /\n",
    "\n",
    "#list file at hdfs home directory\n",
    "hadoop fs -ls\n",
    "```\n",
    "\n",
    "* uploading / downloading from local filesystem\n",
    "![](img_midterm/cpfromtolocal.png)\n",
    "\n",
    "* deletion\n",
    "> Hadoop won't overwrite files and directories. This feature helps protect users from accidentally replacing data that may have taken hours to produce. If you need to replace a file or directory in HDFS, you must first remove the existing one. \n",
    "\n",
    "```\n",
    "#remove a file\n",
    "$ hadoop fs -rm /dualcore/example.txt\n",
    "```\n",
    "* create/remove directory\n",
    "\n",
    "```\n",
    "#this will create a dir testlog at the home directory\n",
    "hadoop fs -mkdir testlog\n",
    "\n",
    "#remove a directory (recursively):\n",
    "hadoop fs -rm -r /dualcore/example/\n",
    "```\n",
    "* sample data (e.g., head/tail)(hw1 15)\n",
    "\n",
    "```\n",
    "hadoop fs -cat latlon/latlon.tsv | head -n 10\n",
    "```\n",
    "\n",
    "* viewing data (cat) (hw1 15)\n",
    "\n",
    "```\n",
    "hadoop fs -cat latlon/latlon.tsv\n",
    "```\n",
    "* extract data (get and getmerge)(hw1 18)\n",
    "\n",
    "```\n",
    "#HW1 18: Download latlon.tsv from HDFS as latlon_hdfs.tsv in the local folder ADIR/data\n",
    "\n",
    "hadoop fs -get latlon/latlon_hdfs.tsv ADIR/data/latlon_hdfs.tsv\n",
    "\n",
    "#HW1 19: Download the entire content of the latlon folder in HDFS as a single local file latlon_all_hdfs.tsv in the folder ADIR/data\n",
    "\n",
    "hadoop fs -getmerge latlon latlon_all_hdfs.tsv\n",
    "\n",
    "```\n",
    "* count # of rows, **columns**\n",
    "\n",
    "```\n",
    "\n",
    "hadoop fs -cat /path/to/hdfs/filename | wc -l\n",
    "\n",
    "```\n",
    "unzip and upload to hdfs. (hw1)\n",
    "\n",
    "```\n",
    "#gunzip is used for uncompress file compressed by gzip command, the file name ends with .gz, \n",
    "#The -c option to gunzip uncompresses to standard output, and the dash (-) in the hadoop fs -put command takes whatever is being sent to its standard input and places that data in HDFS.\n",
    "\n",
    "gunzip -c ~/training_materials/developer/data/access_log.gz \\\n",
    "| hadoop fs -put - weblog/access_log\n",
    "\n",
    "#unzip is used for uncompress file compressed by zip command, the file name ends with .zip, \n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MapReduce"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
