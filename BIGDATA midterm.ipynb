{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Big data: Conceptual: \n",
    "* Concept of Big Data:\n",
    "\n",
    "\n",
    "* drivers of big data\n",
    "\n",
    "* five Vs\n",
    "    * Volume, \n",
    "        * dataset bewteen Terabyte and Petabyte is \"big\"\n",
    "        * opportunity: large volume raise the chance of getting better data\n",
    "        * challenge: reading data takes a lot of time (parallel computing using Hadoop)\n",
    "    * Velocity (Data generated/processed/analyzed per second/hour/day)\n",
    "        * Speed at which data is created, processed and analyzed continues to accelerate\n",
    "        * challenge: processing log data (streaming analytics using Spark)\n",
    "    * Variety: complexity of data types,and sources\n",
    "        * different structure of data\n",
    "        * Internal source: transactional systems,server logs, emails\n",
    "        * External: social media, sensor networks, weather data\n",
    "        * Opportunity: Get high quality data, Big data provides a way to capture novel data sources like social media,click stream\n",
    "        * Challenge: traditional technologies are not flexible to deal with large variety of semi- or un-structured data (80% are unstructured, while 20% are structured.)\n",
    "    * Verasity: data uncertainty. Large volumes means higher risks of getting incorrect information. Incorrectly indexed data or spelling mistake could make whole dataset useless.\n",
    "    * Value: valuable application.\n",
    "        * As volumn increases, value of each extra data point decrease.\n",
    "        * As variety of data available increases, not all data may have value.\n",
    "\n",
    "![img_5v](img_midterm\\5v.png)\n",
    "\n",
    "\n",
    "* different data structure\n",
    "    * structured: transactional\n",
    "    * Semi-structured: sensor data,logs,RFID\n",
    "    * Unstructured: reviews, texts, audio,video\n",
    "    \n",
    "    \n",
    "* data size units\n",
    "![datasize](img_midterm\\datasize.png)\n",
    "\n",
    "\n",
    "* the data transfer problem, and how/why big data can help.\n",
    "![tranfer1](img_midterm\\datatransfer1.png)\n",
    "transfer\n",
    "![tranfer2](img_midterm\\datatransfer2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Linux OS: \n",
    "* linux command structure:\n",
    "![lcs](img_midterm/lcs.png)\n",
    "\n",
    "linux file structure: Tree structure,\n",
    "* root: /\n",
    "* home: ~ (default directory)\n",
    "* parent dir: ../\n",
    "\n",
    "\n",
    "![](img_midterm/linux_structure.png)\n",
    "\n",
    "\n",
    "\n",
    "* relative and absolute path:\n",
    "\n",
    "    * abs path: start with root(/) \n",
    "```\n",
    "cd /xx\n",
    "```\n",
    "\n",
    "    * relative path: no /\n",
    "```\n",
    "cd xx\n",
    "```\n",
    "\n",
    "* pipe and redirection\n",
    "\n",
    "![](img_midterm/pipe.png)\n",
    "\n",
    " - Shell: interactive command interpreter environment (command line interface) for access to an operating system\n",
    " - Terminal: a terminal window allow users to access to shell. multiple terminal window can access to the local machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LINUX (Hands on) (See Linux.pdf instead)\n",
    "\n",
    "* file listing\n",
    "![](img_midterm/ls.png)\n",
    "* change directories\n",
    "![](img_midterm/cd.png)\n",
    "\n",
    "* copy \n",
    "![](img_midterm/cp.png)\n",
    "* move \n",
    "![](img_midterm/mv.png)\n",
    "* delete\n",
    "![](img_midterm/rm.png)\n",
    "* find \n",
    "![](img_midterm/find.png)\n",
    "* create/remove directories, \n",
    "![](img_midterm/mkdir.png)\n",
    "* sample/view large text files, \n",
    "![](img_midterm/view_larger.png)\n",
    "\n",
    "\n",
    "* pipe \n",
    "* redirection, \n",
    "* head, tail, \n",
    "![](img_midterm/head.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro to Hadoop Conceptual (topic2-hadoop): \n",
    "\n",
    "* motivation for Hadoop \n",
    "    * Storing large scale of data is cheaper than before, but processing data is hard for traditional tools like SQL.\n",
    "        * Hardware failures (how can we reliably store big data at a reasonable cost )\n",
    "        * time needed to read the data into memory for analysis.(how to analyze the big data)\n",
    "    * Hadoop deal with these problem:\n",
    "        * it provides reliable distributed storage, and a general framework for parallel processing at low cost\n",
    "        \n",
    "* core hadoop components (both are scalable, reliable, available, fast)\n",
    "    * Storage: Hadoop Distributed File System (HDFS)\n",
    "        *A framework for distributing data across a cluster\n",
    "    * Processing: MapReduce (A framework for processing data)\n",
    "    * other infrastrucutres to make it work\n",
    "    \n",
    "* Hadoop cluster:\n",
    "    * cluster: a collection of servers running Hadoop software\n",
    "    * nodes: Individual servers within a cluster\n",
    "        * data locality: Each node both stores and processes data\n",
    "* how hadoop achieves scalability, reliability/availability\n",
    "    * scalability: add more nodes to increase scalability (linearly)\n",
    "        * horizontal scaling results in lower cost (run on common hardware)\n",
    "        * horizontal scaling: scale by adding more machine\n",
    "        * Vertical scaling: scale by increasing the capacity of an existing machine\n",
    "    * reliability\n",
    "        * Fault Tolerance below\n",
    "    * **availability**\n",
    "* fault tolerance: \n",
    "![](img_midterm/fault_tolerance.png)\n",
    "    \n",
    "\n",
    "* **differences between Hadoop and RDBMS;**\n",
    "* **advantages & disadvantages of Hadoop**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HDFS (Conceptual) (topic2-hadoop.pdf): \n",
    "\n",
    "* how HDFS organizes data\n",
    "    * provides inexpensive and reliable storage for massive amounts of data\n",
    "    * Like UNIX, tree structure file system\n",
    "        * But no \"current directory\" need to use absolute path\n",
    "    * Optimized for a relatively small number of large files (> 100mb, maybe multiple gbs)\n",
    "![](img_midterm/optimal_for_small_number.png) \n",
    "    \n",
    "* HDFS Architecture:\n",
    "    * Blocks: Smallest unit for read and write. HDFS files are broken into blocks. By default, HDFS uses 128MB blocks.\n",
    "![](img_midterm/blocks.png)\n",
    "    * Master/Slave architecture:\n",
    "![](img_midterm/master_slave.png)\n",
    "![](img_midterm/master_slave_ex.png)\n",
    "    \n",
    "* how data is replicated and distributed within HDFS \n",
    "    * Each blocks is replicated multiple times and stored in different nodes of the cluster.\n",
    "\n",
    "\n",
    "* immutability (cannot change after writing).\n",
    "    * cannot modify files once written, but you can delete and recreate them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HDFS(hands on) (from hw1)\n",
    "* Must use hadoop specific utilities or custom code to access HDFS\n",
    "```\n",
    "hadoop fs\n",
    "```\n",
    "\n",
    "* file listing \n",
    "\n",
    "```\n",
    "# list file at hdfs root directory\n",
    "hadoop fs -ls /\n",
    "\n",
    "#list file at hdfs home directory\n",
    "hadoop fs -ls\n",
    "```\n",
    "\n",
    "* uploading / downloading from local filesystem\n",
    "![](img_midterm/cpfromtolocal.png)\n",
    "\n",
    "* deletion\n",
    "> Hadoop won't overwrite files and directories. This feature helps protect users from accidentally replacing data that may have taken hours to produce. If you need to replace a file or directory in HDFS, you must first remove the existing one. \n",
    "\n",
    "```\n",
    "#remove a file\n",
    "$ hadoop fs -rm /dualcore/example.txt\n",
    "```\n",
    "* create/remove directory\n",
    "\n",
    "```\n",
    "#this will create a dir testlog at the home directory\n",
    "hadoop fs -mkdir testlog\n",
    "\n",
    "#remove a directory (recursively):\n",
    "hadoop fs -rm -r /dualcore/example/\n",
    "```\n",
    "* sample data (e.g., head/tail)(hw1 15)\n",
    "\n",
    "```\n",
    "hadoop fs -cat latlon/latlon.tsv | head -n 10\n",
    "```\n",
    "\n",
    "* viewing data (cat) (hw1 15)\n",
    "\n",
    "```\n",
    "hadoop fs -cat latlon/latlon.tsv\n",
    "```\n",
    "* extract data (get and getmerge)(hw1 18)\n",
    "\n",
    "```\n",
    "#HW1 18: Download latlon.tsv from HDFS as latlon_hdfs.tsv in the local folder ADIR/data\n",
    "\n",
    "hadoop fs -get latlon/latlon_hdfs.tsv ADIR/data/latlon_hdfs.tsv\n",
    "\n",
    "#HW1 19: Download the entire content of the latlon folder in HDFS as a single local file latlon_all_hdfs.tsv in the folder ADIR/data\n",
    "\n",
    "hadoop fs -getmerge latlon latlon_all_hdfs.tsv\n",
    "\n",
    "```\n",
    "* count # of rows, **columns**\n",
    "\n",
    "```\n",
    "\n",
    "hadoop fs -cat /path/to/hdfs/filename | wc -l\n",
    "\n",
    "```\n",
    "unzip and upload to hdfs. (hw1)\n",
    "\n",
    "```\n",
    "#gunzip is used for uncompress file compressed by gzip command, the file name ends with .gz, \n",
    "#The -c option to gunzip uncompresses to standard output, and the dash (-) in the hadoop fs -put command takes whatever is being sent to its standard input and places that data in HDFS.\n",
    "\n",
    "gunzip -c ~/training_materials/developer/data/access_log.gz \\\n",
    "| hadoop fs -put - weblog/access_log\n",
    "\n",
    "#unzip is used for uncompress file compressed by zip command, the file name ends with .zip, \n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MapReduce (conceptual)  (topic2-hadoop.pdf)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Programming model is what the instructions (the language) require to work correctly. Implementation of those instructions are the details that can change without changing the result of following those instructions.\n",
    "    * if my instructions are that you should relocate 3 feet north and 2 feet west you can implement those instructions by moving west first then north, north first then west, or by moving diagonally in a northwest direction. \n",
    "\n",
    "    \n",
    "\n",
    "* concept of map reduce programming model,data locality, share-nothing architecture\n",
    "    * it's not a language, it's a programming model for parallel computing that consists of two functions: map and reduce\n",
    "    * the implementation detects failed tasks and reschedules replacements on machines that are healthy (spare programmer from having to think about failure)\n",
    "    * shared-nothing architecture: tasks have no dependence on one other. Programmer doesn't need to think about the order in which tasks run. \n",
    "    * data locality: Each node both stores and processes data so that it preserve the resource of network bandwidth and increase the throughput(工作量) of the whole big data system.\n",
    "    \n",
    "* articulate the mapreduce workflow (mapper, combiner, shuffle and sort, reducer)\n",
    "    1. Hadoop split a job into many individual map tasks (SPLIT)\n",
    "        * Each task takes a proportion of job input to process in parallel. # of tasks is determined by the amount of input data\n",
    "    2. Mapper takes input and emit output. See detail belows (MAP)\n",
    "    3. (optional) in some case, a (local) combiner is run to reduce the amount of data transferred across the network (COMBINE)\n",
    "        * Some aggregation operations may not have a combiner available (available for SUM and MAX , not available for AVG)\n",
    "    4. Map outputs are shuffled and sorted by key (Shuffle & Sort)\n",
    "        * This involves moving data across nodes\n",
    "    5. Reducer takes input from previous step and emit output. See detail belows (REDUCE)\n",
    "    \n",
    "![](img_midterm/splitex.png)\n",
    "![](img_midterm/shufflesort.png)\n",
    "\n",
    "* Mapper\n",
    "    * MapReduce works with key-value pairs \n",
    "    * Map takes **(k,v) pairs** as input. **One pair at a time (Like multiple lines, one line at a time)**. For each pair,mapper emits a list of output pair (0,1,many)\n",
    "        * \\[key1,value1\\] -> list(\\[key2,value2\\])\n",
    "    * Break down the data\n",
    "        * filter,transform, or parse data (parse stock symbol, price and time from a data feed)\n",
    "        * break a sentence into words\n",
    "* Reducer\n",
    "    * Output from map function becomes the input to the reduce function (after combiner and shuffle /sort)\n",
    "    * Reducer takes a list of values for each key, emits a list of (k,v) pairs (0,1,many)\n",
    "        * (key2,list(value2)) ->  list(\\[key2,value3\\])\n",
    "    * Each job might have multiple reducers, each responsible for a specific key range\n",
    "    * reducer is optional\n",
    "    \n",
    "* role of shuffle and sort\n",
    "    * Organize map outputs for delivery to the reducer (because each reducer only take care of a specific key range)\n",
    "    \n",
    "* advantages and disadvantages.\n",
    "    * advantage\n",
    "![](img_midterm/mapreducebenefit.png)\n",
    "    * **disadvantage (for Hadoop 1)**\n",
    "        * Mapreduce has too high a latency for interactive and streaming applications\n",
    "        * Only support batch processing(批处理):  the computer does several jobs one after the other, without needing instructions between each job\n",
    "        * In Hadoop 1.0, MapReduce does both distributed computing and resource management (via\n",
    "jobtracker)\n",
    "            * Limits availability, scalability, resource utilization, and running non-MapReduce applications. If the job tracker fails, all jobs must restart.\n",
    "        \n",
    "        \n",
    "\n",
    "* Main differences between Hadoop 2.0 and Hadoop 1.0? (somehow relieve the disadvantage above)\n",
    "    * improve nameNode availability\n",
    "    * ability to run non Mapreduce applications(like Spark)\n",
    "    * Beyond batch processing: ability to support interactive and streaming applications\n",
    "    * Separate MapReduce into MR and YARN (Yet Another Resource Negotiator)\n",
    "        * YARN takes over the resource management (CPU, memory etc) and job scheduling, allows multiple kinds of processing to run on a single Hadoop cluster (e.g. batch processing, interactive application, streaming)\n",
    "![](img_midterm/yarn.png)      \n",
    "![](img_midterm/hadoop20.png)      \n",
    "\n",
    "* **Concept of Hadoop streaming**：\n",
    "\n",
    "Hadoop streaming is a utility that comes with the Hadoop distribution. The utility allows you to create and run Map/Reduce jobs with any executable or script (like Python script or Java script) as the mapper and/or the reducer. \n",
    "\n",
    "For example: here -mapper and -reducer are .py file\n",
    "\n",
    "![](img_midterm/hadoop_streaming.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MapReduce (hands on)\n",
    "* create pseudo codes for map and reduce functions for relative simple MapReduceproblems (that can be done with one MapReduce)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hadoop ecosystem: (Conceptual) (topic2-hadoop.pdf)\n",
    "\n",
    "tools for storage, ingestion, processing and data use. Know the\n",
    "roles/use cases of common tools (e.g., sqoop , flume, Kafka , HBase , HDFS,\n",
    "Spark, Pig, Hive, MapReduce , Kudu, Impala, Storm, Flink ); concept of data lake;\n",
    "relationship between Hadoop and traditional data warehouse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sqoop : (Conceptual) \n",
    "\n",
    "Role and advantages of sqoop , whole table versus incremental\n",
    "importing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sqoop: (Hands on) \n",
    "\n",
    "Import MySQL tables into Hadoop or Hive, including using options\n",
    "(e.g., select table, rows, and columns; specify destination location, delimiters,\n",
    "split by field; hive import and options (in the Hive data modeling module))."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
